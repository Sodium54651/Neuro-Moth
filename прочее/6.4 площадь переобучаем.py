import numpy as np 

def Relu(z):
    return np.maximum(0,z)

# делаем производную relu он принимает значения 1 когда больше нуля, 0 когда отрицательно или 0
def  DRelu(z):
    return (z > 0).astype(float)



Data = np.array([
    [2, 2],
    [4, 5],
    [3, 2],
    [2, 3]
])

Answers = np.array([
    [4],
    [20],
    [6],
    [6]
])

# np.random.seed(32)
# теперь запишим значения для 1 ряда весов и платы за воздух
# как обычно 3 на выход или нейрончика 2 что входит
w = np.random.rand(3, 2)
print(w)
# плата за воздух добавляется только к выходным значениям смотри у меня у тебя 3 потому что у тебя 3 нейрончика
bias = np.array([[0.1, 0.1, 0.1]])

# теперь запишим значения для 2 ряда скрытых нейрончиков выходного 1
# мы делаем выход один потому что готовое занчение, и говорим что на вход с прошлого подаётся 
# 3 нейрончика или их выходы, что вышли вот видишь там 3
w1 = np.random.rand(1, 3)
# только одно значение, потомучто работате непосредсвенно с выходными данными
bias1 = np.array([[0.1]])

# z = Data @ w.T + biasd
# прогоняем его, только прогоняем его через отсеиватель неверный неронов, все нейроны что думают неправильно
# отсекаются и да стандартная первая линия
z = Relu(Data @ w.T + bias)
# 2 линия это 1 несчастный нейончик , можно сказть ответ нейросети
NAnswers = z @ w1.T + bias1
# print(z)
# скорость обучения или шаг как тебе удобно тут не тут надо подгонять в большенсве 0.01, но не сегодня
LR = 0.001
# запускем цикл, сколько эпох будет у него что бы обучиться 
for rh in range(10000):
    # расчитываем разницу, насколько мы ошиблись, а так же примерное напрвление куда двигаться
    loss = NAnswers - Answers

    # теперь мы ищем градиент для полследнего выходного слоя
    # print(loss)
    # ты перемножаешь как бы получил его, ответ от прошлого с чем было, и умножаешь на разницу
    # смотри ещё ты берёшь разницу и умножаешь на предыдущее значение, это всёровно что, тебе дали ответ
    # и сказали от ответа к тому что привело, вот такая загагулина должна получиться
    w1G = loss.T @ z
    # w1G = loss.T @ z
    # смотри тут мы делаем сумму по каждой столбику, тобишь проходимся по массиву берём строку, сохраняем 
    # её форму, и сумируем все столбики по вот на элементе по строке и там столбик, короче смотри
    # вот допустим у тебя массив 3 строки на 2 столбика, у тебя получается на выходе будет массив 
    # на выходе с 2 столбиками, и первая сумма будет 1 столбик и сумма всего его столбика всё, и так со всеми
    # оставляя форму массива
    bias1G = np.sum(loss, axis=0, keepdims=True)

    # теперь высиськуем для 1 слоя градинет
    # так смотри сейчас внимательно у тебя считаюстя потери что ты потерял и высчитывается с текущеми весами кто 
    # бзднул, потом ты смотришь кто реально вообдще тянул всю команду на дно, тобишь я про DRelu если что, 
    # все отрицательные и 0 заменяем на 0, а все остальные заменяем 1, что бы просто не трогать, да 
    # так нейросеточа должна полсти ещё быстрее, но только получается что отмирают нейрончики и может быть массив
    # вообще пустой, вообще забитый нулями, но это не сегодня у тебя тут вообще под завязку
    wwG = loss @ w1 * DRelu(z)
    # так нет смотри, ты работаешь тут с чем с данными сырыми вот и бери их и бери то что было Data, 
    # дальше берёшь умножаешь на прошлый полученный слой, на то, что он там накопал, что получилось
    # мы его приняли как ответ, и уже от обратно высиськовываем, с другми последующими слоями должно быть
    # так же но ты уже там сам
    # print(Data)
    # print(wwG)
    wG = Data.T @ wwG
    # весы ну ты знаешь берём 1 строчку и каждый её столбик суммируем
    biasG = np.sum(wwG, keepdims=True, axis=0)

    # print(w1G)
    # print(w1)
    # теперь дополним значения в весах для скрытых слоёв
    w1 -= w1G * LR
    bias1 -= bias1G * LR 

    w -= wG.T * LR
    bias -= biasG * LR

    # проводим тестирование 
    z = Relu(Data @ w.T + bias)
    # z = Data @ w.T + bias
    NAnswers = z @ w1.T + bias1
    # print(NAnswers)

print(NAnswers)
Data = np.array([
    [6, 6],
    [1, 2],
    [8, 8],
    [7, 5]
])

Answers = np.array([
    [36],
    [2],
    [64],
    [35]
])

z = Relu(Data @ w.T + bias)
# 2 линия это 1 несчастный нейончик , можно сказть ответ нейросети
NAnswers = z @ w1.T + bias1
print(NAnswers)
