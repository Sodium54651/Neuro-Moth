import numpy as np
# добавляем фукцнию отсеивания низкоуровневых нейрончиков или тех что не работают
# это нужно делать только в скрытых слоях в выходном нужно производную сасать
# данная функция возвращает само значение или 0 если оно меньше нуля
def Relu(z):
    return np.maximum(z, 0)

def DRelu(z):
    return (z > 0).astype(float)

# для начала введём начальные данные для обучения
Data = np.array([
    [1, 2],
    [2, 3],
    [3, 4],
    [4, 5],
    [5, 6]
])
# далее запишим ответы для сверяния 
Answers = np.array([
    [5],
    [8],
    [11],
    [14],
    [17]
])

# дальше необходимо нормализовать данные, я подсмотрю в другую работу где я выполнял нормализацию
# следовательно мы выбираем фукнцию, что будет сомтреть наш массив и находить его максимальный в 
# где массиве Data, что искать среди столбиков axis 0
maxData = np.max(Data, axis=0)
# далее делим построчно ну по столбично, все данные на те, что там есть максимальный
NormData = Data / maxData

# дальше необходимо просмотреть сколько примерно будет нейронов, до этого нейроны считались 
# на рандом, теперь попробуем считать по действиям, всего дейсвтий 3 следовательно должно быть по 3 ряда скрытых слоёв 
# не считая выходной слой 4 скрытых слоя считая выходной, будем идти от обратного, из головы, 2 > 3 > 4 > 3 > 2 = 1
# за слои я считаю не сами нейроны а их пути к скольким различным путям нужно идти одному нейрону 
# всего путей

# подготовим первые весы выходные данные входные
w1 = np.random.rand(3, 2)
# далее необходимо добавить плату за воздух, что будет просто смещать значения
b1 = np.array([[0.1, 0.1, 0.1]])

# далее сделаем следующий нейрончик что будет принимать уже 3 слоя и высерать 4
w2 = np.random.rand(4, 3)
# далее создаём весы для 4 выходных
b2 = np.array([[0.1, 0.1, 0.1, 0.1]])

# создаём следующий слой для того, да это скорее всего слой, а нейрончики это данные 
# что получаются в результате этого слоя, где будет на выход 3 и вход 4
# хотя погоди смотри у меня 2 действия умножения и плюсования, суммирования о какие слова я знаю
# значит у меня будет 2 дейсвтия это умножение и сложение, что уже поджёвано, следовательно, это будет 
# выходной слой, где 4 устремятся в 1, но нужно будет сделать спуск, что бы было красиво
# как было нарисовано на картиночках мало много мало выход, ладно у меня будет не так как у всех
w3 = np.random.rand(1, 4)
b3 = np.array([[0.1]])


# данные прописали, осталось теперь решить перый раз, что бы посмотреть насколько мы попали
# для этого, мы вводим функцию, что будет отсеивать наши значения будем использовать Relu
# мы берём наши данные перемножаем эта значения на наши веса, и мы по и тогу получим 
# размерность нашиз весов, что бы получить выходное значение по 3 штучки и добавим сместители
# плату за воздух
# Relu я использую потому что фукнция простая ну и хватит, значит
n1 = Relu(Data @ w1.T + b1)
# далее по результатам с прошлого нейрончика попробуем жевать следующее решение так же 
n2 = Relu(n1 @ w2.T + b2)
#?  дале делаем это уже для выходного слоя
n3 = Relu(n2 @ w3.T + b3)

# теперь попробуем прогнать что есть, но теперь нужно умножить на максимальную
# хотя получилось, что у нейросети с первого раза получилось +- нажевать 
# ответ, неплохо, нормализованные данные не нужно обратно нажёвывать
# нормализацию обратно вставить я не знаю как, но без неё пока всё не плохо
print(n3)


# далее начнём обучать с шагом 0.01
LR = 0.01
# сделаем счётчик что бы отслеживать сколько было действий
rh = 0
nrh = np.array
# я попробую сделать с проверкой отличается ли значение хоть на сколько то, это будет немного эффективней
# возможно
# while ((n3 - Answers).abs() <= 0.5 or nrh == n3):
# while (nrh == n3):
for rh in range(10000):
    rh += 1
    # дальше нужно посчитать потерю, что есть, что бы знать куда двигаться, как при методе дихотомии
    loss = Answers - n3
    # дальше считаем уже то насколько промазал последний слой 
    # и умножаем на DRelu это образующее Relu решито, что пропускает только 
    # то, что больше нуля, у меня не было изменено, здесь проблема может быть в этом
#?    сюда он пихает ответ, что он высрал это получается итоговый ответ
    w3G = loss.T @ n3

    # далее берём весы и у меня в прошлой штуке, берём сумму общую, и смещаем
    # на единицу смешения 
    b3G = np.sum(loss, axis=0, keepdims=True)

    # дальше находим следующии, элемент по токому же примеру
    # так у меня тут какая то странность я тут обрабрабатываю DRelu а так нет
    # то есть первую не делал а последующие делал вот так промежуточный и 
    # уже искал так попробуем 
#? сейчас мы ищем потерю относительно нового слоя 
    ww2 = loss @ w3 * DRelu(n2)
    # перемножаем ресурсы на ошибку, что бы понять насколько ответ был ошибочен
    w2G = n1.T @ ww2
    b2G = np.sum(ww2, axis=0, keepdims=True)
    # мы продолжаем умножать на общую потерю, это нужно для того, что бы 
    # точно определять на сколько он влияет на общий процесс, так он 
    # будет высиськовывать точечные нейрончики, что а после перемножается
    # на то сколько правильно он нажевал свой ответик личный

#?    общая ошибка @ кто это принял * DRelu результат тут скорее всего ошибка имеется
    # введу та что есть с прошлого и её опять нужно высчитывать а ответ правильных 
    # не понятно где брать
    ww1 = loss @ w2 * DRelu(n1)
    # данные с которыми работали.T @ примерная ошибка ответа
    w1G = Data.T @ ww1
    b1G = np.sum(ww1, axis=0, keepdims=True)


    # теперь отнять неправильные значения что мы нашли, ошибочными с разницой, 
    # но применим на всякий с шаком
    w3 -= w3G * LR
    b3 -= b3G * LR

    w2 -= w2G * LR
    b2 -= b2G * LR

    w1 -= w1G * LR
    b1 -= b1G * LR

    n1 = Relu(Data @ w1.T + b1)
    n2 = Relu(n1 @ w2.T + b2)
    nrh = n3
    n3 = Relu(n2 @ w3.T + b3)

print(rh + "\n")
print(n3)
    














