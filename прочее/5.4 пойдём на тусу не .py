import numpy as np 
# прописываем фукнции, так как всё будет использоваться не один раз а писать несколько раз одно и тоже не
# нахожедние где все значения меньше нуля и обнуления их
def Relu(z):
    return np.maximum(0, z)

# выдача выходного значения основанное на предыдущих ответах
def Sigmoid(a):
    return 1 / (1 + np.exp(- a))

def DSigmoid(a):
    s = Sigmoid(a)
    return s * (1 - s)

# для начала воодим данные 
Data = np.array([
    [9, 2, 1, 1],  # Устал сильно, друзей мало, погода плохая, пельмени есть → 0
    [2, 8, 9, 0],  # Не устал, друзья есть, погода супер, пельменей нет → 1
    [5, 1, 2, 1],  # Середнячок, но всё не очень, есть пельмени → 0
    [1, 7, 7, 0],  # Не устал, куча друзей, хорошая погода, но нет пельменей → 1
])
Answers = np.array([
    [0],
    [1],
    [0],
    [1]
])

# рандомим значения нам понадобится по 4 из каждых для всего 5 нейрончиков включая выходной

w1 = np.random.rand(5, 4)
b1 = np.random.rand(1, 5)

w2 = np.random.rand(4, 4)
b2 = np.random.rand(1, 4)

w3 = np.random.rand(3, 4)
b3 = np.random.rand(1, 3)

w4 = np.random.rand(2, 4)
b4 = np.random.rand(1, 2)

w5 = np.random.rand(1, 4)
b5 = np.random.rand(1, 1)

# теперь задаём какой будет примерно шаг в обучении
lr = 0.01
# ставим в духовку на 2000 циклов
for rh in range(2000):
    # теперь проделываем теже опирации с нейронами что делали раньше

    z1 = Data @ w1.T + b1
    a1 = Relu(z1)
    print(a1)

    z2 = a1 @ w2.T + b2
    a2 = Relu(z2)

    z3 = a2 @ w3.T + b3
    a3 = Relu(z3)

    z4 = a3 @ w4.T + b4
    a4 = Relu(z4)

# в конце записываем что получилось в процессе вот этого алгоритма
    z5 = a4 @ w5.T + b5
    a5 = Sigmoid(z5)
# print(outing)

# смотрим потери как близко мы приблизились и сколько осталось до првильного ответа
    loss = np.mean((a5 - Answers) ** 2)
print(loss)

# теперь смотрим насколько мы ошиблись и вычисляем уменьшаем эту ошибочность на параметр 0.01 что был изначально
# не не я пока не готов тут слишком много всего я пока не понял как это всё работает

# пока не сегодня