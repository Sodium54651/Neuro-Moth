import numpy as np 

# если наш нейрончив в целом гдето меньше нуля то это плохо, и он его обнуляем я пока не знаю как эта
# штука тут работает, но она обрабатывает вектор или матрицу, на признак того что там ноль, а помто возвращает
# все значения что больше нуля даже в матрице он просто заменяет все значения меньше нуля на 0 вот так то 
def Relu(z):
    return np.maximum(0, z)


# Температура, Солнце, Свободное время
Data = np.array([
    [15, 0.9, 2],   # солнечно, есть время — идти гулять
    [5, 0.2, 1],    # холодно, темно — сиди дома
    [25, 1.0, 3],   # идеально
    [10, 0.3, 0],   # вроде норм, но нет времени
    [30, 0.8, 4],   # супер!
])
# 1 — гулять, 0 — не гулять
Target = np.array([[1], [0], [1], [0], [1]])

# задаём цену за воздух рандом
bias = np.array([-0.35, 0.74])
# задаём висы для обоих нейроно на рандом
w = np.array([
    [0.35, 0.45, 0.64],
    [0.45, 0.63, 0.45]
])


# это получается 1 нейрон куда подаётся наши данные тут я просто побаловался с трансвестированием
# этот нейрон занимается прямо обработкой и вычисляет все значения
# как я понимаю мы будем учить только вот этот 1 нейрон ну может ещё снизу тоже
# z1 = Data @ w.T + bias

z1 = (w @ Data.T).T + bias
print(z1)
# print(w.T)
# делаем проверку на адекватность что он в пределах разумного
# print(Relu(z1))
A1 = Relu(z1)
# print(w[1])
print(A1)
# теперь делаем 2 нейрон уже выходной и сдвигаем данные на 2 слой
# тут я перепутал надо было вставить данные что да, но так он не запусается почему то массив как то изменяется
z2 = A1.T @ w[1] + bias[1]
# это готовый ответ что он готов поделиться с миром и что он думает о этих данных
# exp делает магию сначала длинную потом короткую, если слишком большое значение он делает так что бы 
# оно стремилось к нулю и потом мы к нему прибалвяем единицу что бы не совсмем неебически маленькие значения
# получились, и обратно если значение неебически малеьное то он пересиськвыавет близе к единице как то там 
# магией но при условии что это вся фигня была меньше нуля
print(1 / (1 + np.exp(- z2)))




